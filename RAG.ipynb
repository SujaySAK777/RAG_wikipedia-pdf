{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "45rDiOLign7_"
      },
      "outputs": [],
      "source": [
        "!pip install -q faiss-cpu gradio wikipedia pypdf sentence-transformers transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import wikipedia\n",
        "from pypdf import PdfReader\n",
        "import gradio as gr\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ksAmgkC_hUwZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=(\"cuda\" if device==0 else \"cpu\"))\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8MENusBhmqK",
        "outputId": "179bba2b-a52d-4f13-d6d2-6fe4bc39839d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INDEX = None           # FAISS index\n",
        "DOCS = []              # list[str] chunks\n",
        "CHUNK_SIZE = 450       # words per chunk (keep modest for prompt size)\n",
        "CHUNK_OVERLAP = 60     # words overlap\n",
        "TOP_K = 4              # retrieved chunks per question\n",
        "HISTORY_TURNS = 4      # how many turns of chat history to feed into prompt"
      ],
      "metadata": {
        "id": "7o6fWrVkh4pz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    words = (text or \"\").split()\n",
        "    chunks = []\n",
        "    step = max(1, chunk_size - overlap)\n",
        "    for i in range(0, len(words), step):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def build_faiss_index(chunks):\n",
        "    # SentenceTransformer returns float32 by default; ensure np.float32 for FAISS\n",
        "    embeddings = embedder.encode(chunks, convert_to_numpy=True, normalize_embeddings=False)\n",
        "    embeddings = embeddings.astype(np.float32)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def retrieve(question, k=TOP_K):\n",
        "    global INDEX, DOCS\n",
        "    if INDEX is None or not DOCS:\n",
        "        return []\n",
        "    q_emb = embedder.encode([question], convert_to_numpy=True, normalize_embeddings=False).astype(np.float32)\n",
        "    D, I = INDEX.search(q_emb, k)\n",
        "    return [DOCS[i] for i in I[0] if 0 <= i < len(DOCS)]\n",
        "\n",
        "def format_prompt(history, retrieved_chunks, question):\n",
        "    # keep last few turns to help follow-ups\n",
        "    history_str = \"\"\n",
        "    if history:\n",
        "        trimmed = history[-HISTORY_TURNS:]\n",
        "        for u, a in trimmed:\n",
        "            history_str += f\"User: {u}\\nAssistant: {a}\\n\"\n",
        "\n",
        "    context = \"\\n\\n---\\n\".join(retrieved_chunks)\n",
        "\n",
        "    conversation_part = f\"Conversation so far:\\n{history_str}\" if history_str else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant that must answer ONLY using the provided context. \"\n",
        "        \"If the answer cannot be found in the context, say you don't know.\\n\\n\"\n",
        "        f\"{conversation_part}\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"User question: {question}\\n\\n\"\n",
        "        \"Give a concise, accurate answer grounded strictly in the context.\"\n",
        "    )\n",
        "    return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "K3O4qKych7d1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(file):\n",
        "    \"\"\"Load a PDF file, build chunks+index.\"\"\"\n",
        "    global INDEX, DOCS\n",
        "    if file is None:\n",
        "        return \"⚠️ Please upload a PDF first.\"\n",
        "    text = \"\"\n",
        "    reader = PdfReader(file.name)\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text() or \"\"\n",
        "        text += page_text + \"\\n\"\n",
        "    DOCS = split_text(text)\n",
        "    if not DOCS:\n",
        "        INDEX = None\n",
        "        return \"⚠️ Could not extract text from the PDF.\"\n",
        "    INDEX = build_faiss_index(DOCS)\n",
        "    return f\"✅ Loaded PDF with {len(DOCS)} chunks.\"\n",
        "\n",
        "def load_wikipedia(topic):\n",
        "    \"\"\"Load a Wikipedia topic, build chunks+index.\"\"\"\n",
        "    global INDEX, DOCS\n",
        "    topic = (topic or \"\").strip()\n",
        "    if not topic:\n",
        "        return \"⚠️ Enter a Wikipedia topic.\"\n",
        "    try:\n",
        "        page = wikipedia.page(topic)\n",
        "        text = page.content\n",
        "    except wikipedia.DisambiguationError as e:\n",
        "        return f\"⚠️ Multiple pages found. Try a more specific title. Examples: {e.options[:8]}\"\n",
        "    except wikipedia.PageError:\n",
        "        return \"⚠️ Page not found. Try another title.\"\n",
        "    DOCS = split_text(text)\n",
        "    if not DOCS:\n",
        "        INDEX = None\n",
        "        return \"⚠️ No content found on that page.\"\n",
        "    INDEX = build_faiss_index(DOCS)\n",
        "    return f\"✅ Loaded Wikipedia article with {len(DOCS)} chunks.\"\n"
      ],
      "metadata": {
        "id": "gl06STX8iYkI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_respond(message, history):\n",
        "    if INDEX is None or not DOCS:\n",
        "        return \"⚠️ First load a PDF or a Wikipedia article (left panel).\"\n",
        "    retrieved = retrieve(message, k=TOP_K)\n",
        "    if not retrieved:\n",
        "        return \"I don't know based on the available context.\"\n",
        "    prompt = format_prompt(history, retrieved, message)\n",
        "    out = generator(prompt, max_length=256, do_sample=False)[0][\"generated_text\"].strip()\n",
        "    return out"
      ],
      "metadata": {
        "id": "bM73j5qjiduE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 📚 RAG Chatbot — PDF / Wikipedia\\nAnswers are grounded **only** in the loaded content.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Load Source\")\n",
        "\n",
        "            pdf = gr.File(label=\"Upload a PDF\", file_types=[\".pdf\"])\n",
        "            pdf_status = gr.Textbox(label=\"PDF Status\", interactive=False)\n",
        "            pdf.upload(load_pdf, inputs=pdf, outputs=pdf_status)\n",
        "\n",
        "            gr.Markdown(\"**— or —**\")\n",
        "\n",
        "            wiki_box = gr.Textbox(label=\"Wikipedia Topic (e.g., 'Python (programming language)')\")\n",
        "            wiki_status = gr.Textbox(label=\"Wikipedia Status\", interactive=False)\n",
        "            wiki_box.submit(load_wikipedia, inputs=wiki_box, outputs=wiki_status)\n",
        "\n",
        "            gr.Markdown(\"Tip: After loading, switch to the chat on the right.\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### Chat\")\n",
        "            chatbot = gr.Chatbot(height=420)\n",
        "            msg = gr.Textbox(placeholder=\"Ask a question about the loaded content…\")\n",
        "            clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "            def user_submit(user_message, chat_history):\n",
        "                # Append user message; assistant reply computed next\n",
        "                return \"\", chat_history + [[user_message, None]]\n",
        "\n",
        "            def bot_reply(chat_history):\n",
        "                user_message = chat_history[-1][0]\n",
        "                answer = chat_respond(user_message, chat_history[:-1])\n",
        "                chat_history[-1][1] = answer\n",
        "                return chat_history\n",
        "\n",
        "            # Wire events\n",
        "            msg.submit(user_submit, [msg, chatbot], [msg, chatbot]).then(\n",
        "                bot_reply, chatbot, chatbot\n",
        "            )\n",
        "            clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"—\\n**Notes**: Uses `all-MiniLM-L6-v2` for embeddings + `flan-t5-base` for answers. \"\n",
        "        \"If the answer isn't in the context, the assistant will say it doesn't know.\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkqFKiMoihJN",
        "outputId": "735cfeb2-8ce1-4d43-8565-7b631ebd0c41"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-801447556.py:22: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=420)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "sVrMxD3CilqN",
        "outputId": "c9a8394a-843f-42ae-fae4-cd473e187f03"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3594bce44ee6654a90.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3594bce44ee6654a90.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}